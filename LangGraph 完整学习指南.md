# LangGraph å®Œæ•´å­¦ä¹ æŒ‡å—ï¼šä»å…¥é—¨åˆ°å®è·µ

## ğŸ“š ç›®å½•

1. [LangGraphåŸºç¡€æ¦‚å¿µä»‹ç»](#1-langgraphåŸºç¡€æ¦‚å¿µä»‹ç»)
2. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#2-æ ¸å¿ƒç»„ä»¶è¯¦è§£)
3. [å®‰è£…å’Œç¯å¢ƒé…ç½®](#3-å®‰è£…å’Œç¯å¢ƒé…ç½®)
4. [æ¸è¿›å¼ç¤ºä¾‹é¡¹ç›®](#4-æ¸è¿›å¼ç¤ºä¾‹é¡¹ç›®)
5. [å®é™…åº”ç”¨åœºæ™¯](#5-å®é™…åº”ç”¨åœºæ™¯)
6. [æœ€ä½³å®è·µå’Œé«˜çº§ç‰¹æ€§](#6-æœ€ä½³å®è·µå’Œé«˜çº§ç‰¹æ€§)
7. [å­¦ä¹ è·¯å¾„å»ºè®®](#7-å­¦ä¹ è·¯å¾„å»ºè®®)

---

## 1. LangGraphåŸºç¡€æ¦‚å¿µä»‹ç»

### 1.1 ä»€ä¹ˆæ˜¯LangGraphï¼Ÿ

LangGraphæ˜¯ä¸€ä¸ªä½çº§åˆ«çš„ç¼–æ’æ¡†æ¶å’Œè¿è¡Œæ—¶ï¼Œä¸“é—¨ç”¨äºæ„å»ºã€ç®¡ç†å’Œéƒ¨ç½²é•¿æœŸè¿è¡Œã€æœ‰çŠ¶æ€çš„æ™ºèƒ½ä½“ã€‚å®ƒæ˜¯LangChainç”Ÿæ€ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†ä¹Ÿå¯ä»¥ç‹¬ç«‹ä½¿ç”¨ã€‚

LangGraphçš„æ ¸å¿ƒä¼˜åŠ¿åŒ…æ‹¬ï¼š
- **æŒä¹…åŒ–æ‰§è¡Œ**ï¼šæ„å»ºèƒ½å¤Ÿåœ¨å¤±è´¥æ—¶æŒä¹…åŒ–å¹¶é•¿æœŸè¿è¡Œçš„æ™ºèƒ½ä½“
- **äººæœºäº¤äº’**ï¼šåœ¨ä»»ä½•ç‚¹æ£€æŸ¥å’Œä¿®æ”¹æ™ºèƒ½ä½“çŠ¶æ€ï¼Œå®ç°äººå·¥ç›‘ç£
- **å…¨é¢è®°å¿†**ï¼šåˆ›å»ºå…·æœ‰çŸ­æœŸå·¥ä½œè®°å¿†å’Œé•¿æœŸè®°å¿†çš„æœ‰çŠ¶æ€æ™ºèƒ½ä½“
- **è°ƒè¯•èƒ½åŠ›**ï¼šé€šè¿‡LangSmithè·å¾—å¤æ‚æ™ºèƒ½ä½“è¡Œä¸ºçš„æ·±åº¦å¯è§æ€§
- **ç”Ÿäº§å°±ç»ª**ï¼šéƒ¨ç½²å¤æ‚çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå…·å¤‡å¯æ‰©å±•çš„åŸºç¡€è®¾æ–½

### 1.2 LangGraphä¸LangChainçš„å…³ç³»

è™½ç„¶LangGraphå¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼Œä½†å®ƒä¸LangChainäº§å“æ— ç¼é›†æˆï¼Œä¸ºå¼€å‘è€…æä¾›æ„å»ºæ™ºèƒ½ä½“çš„å…¨å¥—å·¥å…·ï¼š

- **LangChain**ï¼šæä¾›é›†æˆå’Œå¯ç»„åˆç»„ä»¶ï¼Œç®€åŒ–LLMåº”ç”¨å¼€å‘
- **LangSmith**ï¼šè¿½è¸ªè¯·æ±‚ã€è¯„ä¼°è¾“å‡ºã€ç›‘æ§éƒ¨ç½²çš„ä¸€ç«™å¼å¹³å°
- **LangGraph**ï¼šä¸“é—¨ç”¨äºé•¿æœŸè¿è¡Œã€æœ‰çŠ¶æ€å·¥ä½œæµçš„éƒ¨ç½²å¹³å°

### 1.3 æ ¸å¿ƒç‰¹æ€§

LangGraphä¸“æ³¨äºæ™ºèƒ½ä½“ç¼–æ’çš„åŸºç¡€èƒ½åŠ›ï¼š
- **çŠ¶æ€ç®¡ç†**ï¼šç»Ÿä¸€çš„å…±äº«çŠ¶æ€å¯¹è±¡ï¼Œæ‰€æœ‰èŠ‚ç‚¹è¯»å†™åŒä¸€çŠ¶æ€
- **å›¾ç»“æ„**ï¼šåŸºäºèŠ‚ç‚¹ï¼ˆNodesï¼‰å’Œè¾¹ï¼ˆEdgesï¼‰çš„å·¥ä½œæµå»ºæ¨¡
- **æ¡ä»¶æ‰§è¡Œ**ï¼šæ”¯æŒåŸºäºçŠ¶æ€çš„æ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯
- **æ¶ˆæ¯ä¼ é€’**ï¼šçµæ´»çš„æ¶ˆæ¯å¤„ç†å’Œä¼ é€’æœºåˆ¶

---

## 2. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 2.1 Stateï¼ˆçŠ¶æ€ï¼‰

Stateæ˜¯LangGraphä¸­æœ€é‡è¦çš„æ¦‚å¿µä¹‹ä¸€ï¼Œè¡¨ç¤ºåº”ç”¨ç¨‹åºå½“å‰çš„å¿«ç…§ã€‚å®ƒæ˜¯ä¸€ä¸ªå…±äº«çš„æ•°æ®ç»“æ„ï¼Œè´¯ç©¿æ•´ä¸ªå›¾çš„æ‰§è¡Œè¿‡ç¨‹ã€‚

#### çŠ¶æ€å®šä¹‰

```python
from typing_extensions import TypedDict
from typing import List, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage

# åŸºæœ¬çŠ¶æ€å®šä¹‰
class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    user_input: str
    context: dict

# æˆ–è€…ä½¿ç”¨Pydanticæ¨¡å‹
from pydantic import BaseModel, Field

class AppState(BaseModel):
    messages: List[BaseMessage] = []
    user_input: str = ""
    context: dict = Field(default_factory=dict)
```

#### Reducerå‡½æ•°

Reducerå‡½æ•°å®šä¹‰äº†å¦‚ä½•æ›´æ–°çŠ¶æ€ï¼š

```python
from langgraph.graph.message import add_messages

# add_messagesæ˜¯LangGraphæä¾›çš„reducerå‡½æ•°
# å®ƒä¼šæ™ºèƒ½åœ°åˆå¹¶æ¶ˆæ¯åˆ—è¡¨
class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
```

### 2.2 Nodesï¼ˆèŠ‚ç‚¹ï¼‰

èŠ‚ç‚¹æ˜¯å›¾ä¸­çš„å¤„ç†å•å…ƒï¼Œæ¥æ”¶å½“å‰çŠ¶æ€å¹¶è¿”å›æ›´æ–°åçš„çŠ¶æ€ã€‚

#### åŸºæœ¬èŠ‚ç‚¹åˆ›å»º

```python
def my_node(state: State) -> State:
    """ç®€å•çš„èŠ‚ç‚¹å‡½æ•°"""
    # å¤„ç†é€»è¾‘
    new_message = {"role": "assistant", "content": "Hello from node!"}
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {
        "messages": [new_message],
        "processed": True
    }
```

#### ä¸åŒç±»å‹çš„èŠ‚ç‚¹

1. **å‡½æ•°èŠ‚ç‚¹**ï¼šåŒ…è£…æ™®é€šPythonå‡½æ•°
```python
def process_data(state: State) -> State:
    # æ•°æ®å¤„ç†é€»è¾‘
    result = process_user_input(state["user_input"])
    return {"result": result}
```

2. **LLMèŠ‚ç‚¹**ï¼šå°è£…ä¸è¯­è¨€æ¨¡å‹çš„äº¤äº’
```python
from langchain_openai import ChatOpenAI

def llm_node(state: State) -> State:
    llm = ChatOpenAI()
    messages = state["messages"]
    
    response = llm.invoke(messages)
    return {"messages": [response]}
```

3. **å·¥å…·èŠ‚ç‚¹**ï¼šæä¾›ä¸å¤–éƒ¨ç³»ç»Ÿçš„é›†æˆ
```python
from langgraph.prebuilt import ToolNode
from langchain_community.tools import TavilySearchResults

# åˆ›å»ºå·¥å…·
search_tool = TavilySearchResults()
tools = [search_tool]

# åˆ›å»ºå·¥å…·èŠ‚ç‚¹
tool_node = ToolNode(tools)
```

### 2.3 Edgesï¼ˆè¾¹ï¼‰

è¾¹å®šä¹‰äº†èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å’Œæ‰§è¡Œæµç¨‹ã€‚

#### ç›´æ¥è¾¹ï¼ˆDirect Edgesï¼‰

æœ€ç®€å•çš„è¾¹ç±»å‹ï¼Œç›´æ¥è¿æ¥ä¸¤ä¸ªèŠ‚ç‚¹ï¼š

```python
from langgraph.graph import StateGraph, START, END

# åˆ›å»ºå›¾æ„å»ºå™¨
builder = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
builder.add_node("node_a", node_a_function)
builder.add_node("node_b", node_b_function)

# æ·»åŠ ç›´æ¥è¾¹
builder.add_edge(START, "node_a")  # ä»å¼€å§‹åˆ°node_a
builder.add_edge("node_a", "node_b")  # ä»node_aåˆ°node_b
builder.add_edge("node_b", END)  # ä»node_båˆ°ç»“æŸ
```

#### æ¡ä»¶è¾¹ï¼ˆConditional Edgesï¼‰

åŸºäºçŠ¶æ€æ¡ä»¶å†³å®šä¸‹ä¸€ä¸ªæ‰§è¡Œçš„èŠ‚ç‚¹ï¼š

```python
def routing_function(state: State) -> str:
    """æ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
    if state["user_input"].lower() == "exit":
        return "end_node"
    else:
        return "continue_node"

# æ·»åŠ æ¡ä»¶è¾¹
builder.add_conditional_edges(
    "current_node",
    routing_function,
    {
        "end_node": "end_node",
        "continue_node": "continue_node"
    }
)
```

### 2.4 Graphç¼–è¯‘å’Œæ‰§è¡Œ

```python
# ç¼–è¯‘å›¾
graph = builder.compile()

# æ‰§è¡Œå›¾
result = graph.invoke({
    "messages": [{"role": "user", "content": "Hello!"}],
    "user_input": "Hello!"
})

# æˆ–è€…ä½¿ç”¨æµå¼æ‰§è¡Œ
for step in graph.stream(initial_state):
    print(step)
```

---

## 3. å®‰è£…å’Œç¯å¢ƒé…ç½®

### 3.1 ç¯å¢ƒè¦æ±‚

- Python 3.8+
- pipæˆ–uvåŒ…ç®¡ç†å™¨
- OpenAI APIå¯†é’¥ï¼ˆæˆ–å…¶ä»–LLMæœåŠ¡ï¼‰

### 3.2 å®‰è£…æ­¥éª¤

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv langgraph_env

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows
langgraph_env\Scripts\activate
# macOS/Linux
source langgraph_env/bin/activate

# å®‰è£…LangGraph
pip install -U langgraph

# å®‰è£…LangChainç›¸å…³åŒ…
pip install langchain langchain-openai

# å®‰è£…å…¶ä»–å¸¸ç”¨å·¥å…·
pip install python-dotenv  # ç¯å¢ƒå˜é‡ç®¡ç†
```

### 3.3 ç¯å¢ƒé…ç½®

åˆ›å»º`.env`æ–‡ä»¶ï¼š

```env
OPENAI_API_KEY=your_openai_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here  # å¯é€‰ï¼Œç”¨äºæœç´¢å·¥å…·
```

åœ¨ä»£ç ä¸­åŠ è½½ç¯å¢ƒå˜é‡ï¼š

```python
from dotenv import load_dotenv
import os

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è·å–APIå¯†é’¥
openai_api_key = os.getenv("OPENAI_API_KEY")
```

### 3.4 éªŒè¯å®‰è£…

åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ–‡ä»¶ï¼š

```python
from langgraph.graph import StateGraph, MessagesState, START, END

def test_node(state: MessagesState):
    return {"messages": [{"role": "ai", "content": "LangGraph is working!"}]}

# åˆ›å»ºå›¾
graph = StateGraph(MessagesState)
graph.add_node("test", test_node)
graph.add_edge(START, "test")
graph.add_edge("test", END)
graph = graph.compile()

# æµ‹è¯•æ‰§è¡Œ
result = graph.invoke({"messages": [{"role": "user", "content": "test"}]})
print(result)
```

---

## 4. æ¸è¿›å¼ç¤ºä¾‹é¡¹ç›®

### 4.1 ç¤ºä¾‹1ï¼šHello Worldï¼ˆæœ€ç®€å•çš„å›¾ï¼‰

```python
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

# å®šä¹‰çŠ¶æ€
class State(TypedDict):
    message: str

# å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def hello_node(state: State) -> State:
    return {"message": "Hello, LangGraph!"}

# æ„å»ºå›¾
builder = StateGraph(State)
builder.add_node("hello", hello_node)
builder.add_edge(START, "hello")
builder.add_edge("hello", END)

# ç¼–è¯‘å’Œæ‰§è¡Œ
graph = builder.compile()
result = graph.invoke({"message": ""})
print(result)  # è¾“å‡º: {'message': 'Hello, LangGraph!'}
```

### 4.2 ç¤ºä¾‹2ï¼šç®€å•èŠå¤©æœºå™¨äºº

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

def chatbot_node(state: MessagesState) -> MessagesState:
    """ç®€å•çš„èŠå¤©æœºå™¨äººèŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    
    # è·å–æ¶ˆæ¯å†å²
    messages = state["messages"]
    
    # è°ƒç”¨LLM
    response = llm.invoke(messages)
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {"messages": [response]}

# æ„å»ºå›¾
graph = StateGraph(MessagesState)
graph.add_node("chatbot", chatbot_node)
graph.add_edge(START, "chatbot")
graph.add_edge("chatbot", END)
graph = graph.compile()

# äº¤äº’å¼èŠå¤©
while True:
    user_input = input("User: ")
    if user_input.lower() == "quit":
        break
    
    # æ‰§è¡Œå›¾
    result = graph.invoke({
        "messages": [HumanMessage(content=user_input)]
    })
    
    # è¾“å‡ºAIå›å¤
    ai_response = result["messages"][-1]
    print(f"AI: {ai_response.content}")
```

### 4.3 ç¤ºä¾‹3ï¼šå·¥å…·è°ƒç”¨

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_openai import ChatOpenAI
from langchain_community.tools import TavilySearchResults
from langchain_core.messages import HumanMessage

# å®šä¹‰å·¥å…·
search_tool = TavilySearchResults(max_results=2)
tools = [search_tool]

# åˆ›å»ºå·¥å…·èŠ‚ç‚¹
tool_node = ToolNode(tools)

def call_model(state: MessagesState) -> MessagesState:
    """è°ƒç”¨æ¨¡å‹çš„èŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    llm_with_tools = llm.bind_tools(tools)
    
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    
    return {"messages": [response]}

# æ„å»ºå›¾
graph = StateGraph(MessagesState)
graph.add_node("call_model", call_model)
graph.add_node("tools", tool_node)

# å®šä¹‰è¾¹
graph.add_edge(START, "call_model")
graph.add_conditional_edges(
    "call_model",
    tools_condition,
    {"tools": "tools", END: END}
)
graph.add_edge("tools", "call_model")
graph = graph.compile()

# æµ‹è¯•
result = graph.invoke({
    "messages": [HumanMessage(content="What is the weather in Beijing?")]
})

# æ‰“å°ç»“æœ
for message in result["messages"]:
    if hasattr(message, 'tool_calls') and message.tool_calls:
        print(f"Tool calls: {message.tool_calls}")
    else:
        print(f"Message: {message.content}")
```

### 4.4 ç¤ºä¾‹4ï¼šæ¡ä»¶åˆ†æ”¯

```python
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from langchain_openai import ChatOpenAI

class State(TypedDict):
    user_input: str
    category: str
    response: str

def categorize_node(state: State) -> State:
    """åˆ†ç±»ç”¨æˆ·è¾“å…¥"""
    llm = ChatOpenAI()
    
    prompt = f"""
    è¯·å°†ä»¥ä¸‹ç”¨æˆ·è¾“å…¥åˆ†ç±»ä¸ºï¼šæŠ€æœ¯ã€ç”Ÿæ´»ã€å¨±ä¹ã€å…¶ä»–
    ç”¨æˆ·è¾“å…¥ï¼š{state['user_input']}
    åªéœ€å›ç­”åˆ†ç±»åç§°ã€‚
    """
    
    category = llm.invoke(prompt).content.strip()
    return {"category": category}

def tech_response(state: State) -> State:
    """æŠ€æœ¯ç±»å›å¤"""
    return {"response": "è¿™æ˜¯ä¸€ä¸ªæŠ€æœ¯é—®é¢˜ï¼Œæˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†è§£ç­”..."}

def life_response(state: State) -> State:
    """ç”Ÿæ´»ç±»å›å¤"""
    return {"response": "å…³äºç”Ÿæ´»æ–¹é¢çš„é—®é¢˜ï¼Œæˆ‘çš„å»ºè®®æ˜¯..."}

def entertainment_response(state: State) -> State:
    """å¨±ä¹ç±»å›å¤"""
    return {"response": "å¨±ä¹è¯é¢˜å¾ˆæœ‰è¶£ï¼Œè®©æˆ‘ä»¬èŠèŠ..."}

def default_response(state: State) -> State:
    """é»˜è®¤å›å¤"""
    return {"response": "è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„é—®é¢˜..."}

def route_by_category(state: State) -> str:
    """æ ¹æ®åˆ†ç±»è·¯ç”±åˆ°ä¸åŒçš„å¤„ç†èŠ‚ç‚¹"""
    category = state["category"]
    if "æŠ€æœ¯" in category:
        return "tech_response"
    elif "ç”Ÿæ´»" in category:
        return "life_response"
    elif "å¨±ä¹" in category:
        return "entertainment_response"
    else:
        return "default_response"

# æ„å»ºå›¾
builder = StateGraph(State)
builder.add_node("categorize", categorize_node)
builder.add_node("tech_response", tech_response)
builder.add_node("life_response", life_response)
builder.add_node("entertainment_response", entertainment_response)
builder.add_node("default_response", default_response)

# å®šä¹‰è¾¹
builder.add_edge(START, "categorize")
builder.add_conditional_edges(
    "categorize",
    route_by_category,
    {
        "tech_response": "tech_response",
        "life_response": "life_response",
        "entertainment_response": "entertainment_response",
        "default_response": "default_response"
    }
)
builder.add_edge("tech_response", END)
builder.add_edge("life_response", END)
builder.add_edge("entertainment_response", END)
builder.add_edge("default_response", END)

graph = builder.compile()

# æµ‹è¯•ä¸åŒçš„è¾“å…¥
test_inputs = [
    "Pythonä¸­çš„è£…é¥°å™¨æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
    "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
    "æœ€è¿‘æœ‰ä»€ä¹ˆå¥½çœ‹çš„ç”µå½±å—ï¼Ÿ",
    "å…¶ä»–éšæœºè¯é¢˜"
]

for test_input in test_inputs:
    result = graph.invoke({"user_input": test_input})
    print(f"è¾“å…¥: {test_input}")
    print(f"åˆ†ç±»: {result['category']}")
    print(f"å›å¤: {result['response']}")
    print("-" * 50)
```

### 4.5 ç¤ºä¾‹5ï¼šå¾ªç¯ç»“æ„

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

class LoopState(MessagesState):
    loop_count: int

def assistant_node(state: LoopState) -> LoopState:
    """åŠ©æ‰‹èŠ‚ç‚¹"""
    llm = ChatOpenAI()
    
    system_message = SystemMessage(content="""
    ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚å¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸å¤Ÿæ˜ç¡®ï¼Œè¯·æå‡ºæ¾„æ¸…é—®é¢˜ã€‚
    å¦‚æœç”¨æˆ·çš„é—®é¢˜å·²ç»æ˜ç¡®ï¼Œè¯·ç›´æ¥å›ç­”ã€‚
    """)
    
    messages = [system_message] + state["messages"]
    response = llm.invoke(messages)
    
    return {
        "messages": [response],
        "loop_count": state.get("loop_count", 0) + 1
    }

def should_continue(state: LoopState) -> str:
    """å†³å®šæ˜¯å¦ç»§ç»­å¾ªç¯"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°
    if state.get("loop_count", 0) >= 3:
        return "end"
    
    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æ˜¯é—®é¢˜
    if "?" in last_message.content or "ï¼Ÿ" in last_message.content:
        return "continue"
    else:
        return "end"

# æ„å»ºå›¾
builder = StateGraph(LoopState)
builder.add_node("assistant", assistant_node)

# æ·»åŠ æ¡ä»¶è¾¹å®ç°å¾ªç¯
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    should_continue,
    {
        "continue": "assistant",  # å¾ªç¯å›è‡ªèº«
        "end": END
    }
)

graph = builder.compile()

# æµ‹è¯•å¾ªç¯
print("æ™ºèƒ½åŠ©æ‰‹ï¼šæ‚¨å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ")
initial_state = {
    "messages": [HumanMessage(content="æˆ‘æƒ³äº†è§£æœºå™¨å­¦ä¹ ")],
    "loop_count": 0
}

for step in graph.stream(initial_state):
    if "assistant" in step:
        message = step["assistant"]["messages"][-1]
        print(f"AI: {message.content}")
```

### 4.6 ç¤ºä¾‹6ï¼šå¤æ‚çŠ¶æ€ç®¡ç†

```python
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict, Annotated
from typing import List, Dict, Any
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage

class ComplexState(TypedDict):
    """å¤æ‚çŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[BaseMessage], add_messages]
    user_profile: Dict[str, Any]
    conversation_history: List[Dict[str, Any]]
    current_task: str
    task_progress: Dict[str, float]
    memory: Dict[str, Any]

def profile_manager(state: ComplexState) -> ComplexState:
    """ç”¨æˆ·èµ„æ–™ç®¡ç†èŠ‚ç‚¹"""
    # ä»å¯¹è¯ä¸­æå–ç”¨æˆ·ä¿¡æ¯
    messages = state["messages"]
    last_message = messages[-1]
    
    # ç®€å•çš„ä¿¡æ¯æå–é€»è¾‘
    if "æˆ‘å«" in last_message.content:
        name = last_message.content.split("æˆ‘å«")[-1].strip()
        state["user_profile"]["name"] = name
        state["memory"]["user_name"] = name
    
    return state

def task_manager(state: ComplexState) -> ComplexState:
    """ä»»åŠ¡ç®¡ç†èŠ‚ç‚¹"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # è¯†åˆ«ä»»åŠ¡ç±»å‹
    if "æŸ¥è¯¢" in last_message.content or "æœç´¢" in last_message.content:
        state["current_task"] = "search"
        state["task_progress"]["search"] = 0.0
    elif "åˆ†æ" in last_message.content or "æ€»ç»“" in last_message.content:
        state["current_task"] = "analysis"
        state["task_progress"]["analysis"] = 0.0
    else:
        state["current_task"] = "chat"
        state["task_progress"]["chat"] = 0.0
    
    return state

def response_generator(state: ComplexState) -> ComplexState:
    """å“åº”ç”ŸæˆèŠ‚ç‚¹"""
    llm = ChatOpenAI()
    
    # æ„å»ºç³»ç»Ÿæç¤º
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚
    ç”¨æˆ·ä¿¡æ¯ï¼š{state['user_profile']}
    å½“å‰ä»»åŠ¡ï¼š{state['current_task']}
    ä»»åŠ¡è¿›åº¦ï¼š{state['task_progress']}
    
    è¯·æ ¹æ®ä¸Šä¸‹æ–‡æä¾›é€‚å½“çš„å›å¤ã€‚
    """
    
    messages = [HumanMessage(content=system_prompt)] + state["messages"][-3:]
    response = llm.invoke(messages)
    
    # æ›´æ–°è¿›åº¦
    if state["current_task"] in state["task_progress"]:
        state["task_progress"][state["current_task"]] = 1.0
    
    return {
        "messages": [response],
        "conversation_history": state["conversation_history"] + [
            {
                "task": state["current_task"],
                "progress": state["task_progress"].copy()
            }
        ]
    }

# æ„å»ºå¤æ‚å›¾
builder = StateGraph(ComplexState)
builder.add_node("profile_manager", profile_manager)
builder.add_node("task_manager", task_manager)
builder.add_node("response_generator", response_generator)

# å®šä¹‰æ‰§è¡Œæµç¨‹
builder.add_edge(START, "profile_manager")
builder.add_edge("profile_manager", "task_manager")
builder.add_edge("task_manager", "response_generator")
builder.add_edge("response_generator", END)

graph = builder.compile()

# æµ‹è¯•å¤æ‚çŠ¶æ€ç®¡ç†
test_conversation = [
    "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰",
    "è¯·å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹æœ€è¿‘çš„äººå·¥æ™ºèƒ½æ–°é—»",
    "èƒ½æ€»ç»“ä¸€ä¸‹è¿™äº›æ–°é—»çš„ä¸»è¦å†…å®¹å—ï¼Ÿ"
]

state = {
    "messages": [],
    "user_profile": {},
    "conversation_history": [],
    "current_task": "",
    "task_progress": {},
    "memory": {}
}

for message in test_conversation:
    state["messages"] = [HumanMessage(content=message)]
    result = graph.invoke(state)
    
    print(f"ç”¨æˆ·: {message}")
    print(f"AI: {result['messages'][-1].content}")
    print(f"å½“å‰ä»»åŠ¡: {result['current_task']}")
    print(f"ç”¨æˆ·èµ„æ–™: {result['user_profile']}")
    print(f"ä»»åŠ¡è¿›åº¦: {result['task_progress']}")
    print("-" * 50)
    
    # æ›´æ–°çŠ¶æ€ç”¨äºä¸‹ä¸€è½®
    state = result
```

---

## 5. å®é™…åº”ç”¨åœºæ™¯

### 5.1 å®¢æœæœºå™¨äºº

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.prebuilt import ToolNode
from langchain_community.tools import TavilySearchResults

class SupportState(MessagesState):
    customer_info: dict
    ticket_id: str
    issue_category: str
    resolution_steps: list

def intake_node(state: SupportState) -> SupportState:
    """å®¢æˆ·ä¿¡æ¯æ”¶é›†èŠ‚ç‚¹"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # æå–å®¢æˆ·ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if "å®¢æˆ·ID:" in last_message.content:
        customer_id = last_message.content.split("å®¢æˆ·ID:")[1].strip()
        state["customer_info"] = {"customer_id": customer_id}
        state["ticket_id"] = f"TK{customer_id}{int(time.time())}"
    
    return state

def categorization_node(state: SupportState) -> SupportState:
    """é—®é¢˜åˆ†ç±»èŠ‚ç‚¹"""
    llm = ChatOpenAI()
    messages = state["messages"]
    
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªå®¢æœé—®é¢˜åˆ†ç±»ä¸“å®¶ã€‚è¯·æ ¹æ®å®¢æˆ·æè¿°å°†é—®é¢˜åˆ†ç±»ä¸ºï¼š
    - æŠ€æœ¯æ”¯æŒ
    - è´¦æˆ·é—®é¢˜
    - è®¢å•é—®é¢˜
    - äº§å“å’¨è¯¢
    - å…¶ä»–
    
    åªéœ€å›ç­”åˆ†ç±»åç§°ã€‚
    """
    
    prompt_message = SystemMessage(content=system_prompt)
    response = llm.invoke([prompt_message] + messages[-2:])
    
    state["issue_category"] = response.content.strip()
    return state

def resolution_node(state: SupportState) -> SupportState:
    """é—®é¢˜è§£å†³èŠ‚ç‚¹"""
    llm = ChatOpenAI()
    
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœä»£è¡¨ã€‚
    å®¢æˆ·ä¿¡æ¯ï¼š{state['customer_info']}
    å·¥å•IDï¼š{state['ticket_id']}
    é—®é¢˜åˆ†ç±»ï¼š{state['issue_category']}
    
    è¯·æä¾›ä¸“ä¸šçš„è§£å†³æ–¹æ¡ˆï¼Œæ­¥éª¤è¦æ¸…æ™°è¯¦ç»†ã€‚
    """
    
    messages = [SystemMessage(content=system_prompt)] + state["messages"]
    response = llm.invoke(messages)
    
    # æå–è§£å†³æ­¥éª¤ï¼ˆç®€åŒ–ç‰ˆï¼‰
    steps = response.content.split('\n')
    state["resolution_steps"] = [step.strip() for step in steps if step.strip()]
    
    return {"messages": [response]}

def escalation_node(state: SupportState) -> SupportState:
    """å‡çº§èŠ‚ç‚¹"""
    escalation_message = HumanMessage(content="""
    æ‚¨çš„é—®é¢˜å·²å‡çº§ç»™é«˜çº§æŠ€æœ¯æ”¯æŒå›¢é˜Ÿã€‚
    å·¥å•IDï¼š{}
    æˆ‘ä»¬å°†åœ¨24å°æ—¶å†…ä¸æ‚¨è”ç³»ã€‚
    """.format(state["ticket_id"]))
    
    return {"messages": [escalation_message]}

def should_escalate(state: SupportState) -> str:
    """å†³å®šæ˜¯å¦éœ€è¦å‡çº§"""
    messages = state["messages"]
    conversation_text = " ".join([msg.content for msg in messages])
    
    # ç®€å•çš„å‡çº§é€»è¾‘
    escalation_keywords = ["ç´§æ€¥", "ä¸¥é‡", "æ— æ³•", "æ•…éšœ", "æŠ•è¯‰"]
    if any(keyword in conversation_text for keyword in escalation_keywords):
        return "escalate"
    
    return "resolve"

# æ„å»ºå®¢æœå›¾
builder = StateGraph(SupportState)
builder.add_node("intake", intake_node)
builder.add_node("categorization", categorization_node)
builder.add_node("resolution", resolution_node)
builder.add_node("escalation", escalation_node)

# å®šä¹‰æµç¨‹
builder.add_edge(START, "intake")
builder.add_edge("intake", "categorization")
builder.add_conditional_edges(
    "categorization",
    should_escalate,
    {
        "escalate": "escalation",
        "resolve": "resolution"
    }
)
builder.add_edge("resolution", END)
builder.add_edge("escalation", END)

support_graph = builder.compile()

# æµ‹è¯•å®¢æœç³»ç»Ÿ
print("å®¢æœç³»ç»Ÿæµ‹è¯•")
print("=" * 50)

test_cases = [
    "ä½ å¥½ï¼Œæˆ‘çš„è®¢å•æ˜¾ç¤ºå¼‚å¸¸ï¼Œå®¢æˆ·ID: 12345",
    "ç´§æ€¥ï¼æˆ‘çš„è´¦æˆ·è¢«é”å®šäº†ï¼Œå®¢æˆ·ID: 67890"
]

for test_case in test_cases:
    print(f"å®¢æˆ·è¾“å…¥: {test_case}")
    
    result = support_graph.invoke({
        "messages": [HumanMessage(content=test_case)],
        "customer_info": {},
        "ticket_id": "",
        "issue_category": "",
        "resolution_steps": []
    })
    
    print(f"å·¥å•ID: {result['ticket_id']}")
    print(f"é—®é¢˜åˆ†ç±»: {result['issue_category']}")
    print(f"æœ€åå›å¤: {result['messages'][-1].content}")
    print("-" * 50)
```

### 5.2 ä»£ç åŠ©æ‰‹

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.tools import ShellTool
import ast
import traceback

class CodeAssistantState(MessagesState):
    code_context: dict
    analysis_result: dict
    execution_output: str

def code_analysis_node(state: CodeAssistantState) -> CodeAssistantState:
    """ä»£ç åˆ†æèŠ‚ç‚¹"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # æå–ä»£ç ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if "```python" in last_message.content:
        code = last_message.content.split("```python")[1].split("```")[0].strip()
        
        # åŸºæœ¬è¯­æ³•æ£€æŸ¥
        try:
            ast.parse(code)
            syntax_valid = True
            syntax_error = ""
        except SyntaxError as e:
            syntax_valid = False
            syntax_error = str(e)
        
        state["code_context"] = {
            "code": code,
            "syntax_valid": syntax_valid,
            "syntax_error": syntax_error
        }
        
        state["analysis_result"] = {
            "has_syntax_error": not syntax_valid,
            "error_message": syntax_error if syntax_error else "è¯­æ³•æ£€æŸ¥é€šè¿‡"
        }
    
    return state

def code_execution_node(state: CodeAssistantState) -> CodeAssistantState:
    """ä»£ç æ‰§è¡ŒèŠ‚ç‚¹"""
    if state["analysis_result"]["has_syntax_error"]:
        state["execution_output"] = "ä»£ç æœ‰è¯­æ³•é”™è¯¯ï¼Œæ— æ³•æ‰§è¡Œ"
        return state
    
    code = state["code_context"]["code"]
    
    try:
        # åœ¨å®‰å…¨çš„ç¯å¢ƒä¸­æ‰§è¡Œä»£ç ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼‰
        # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ›´å®‰å…¨çš„æ‰§è¡Œç¯å¢ƒ
        exec(code)
        state["execution_output"] = "ä»£ç æ‰§è¡ŒæˆåŠŸ"
    except Exception as e:
        state["execution_output"] = f"æ‰§è¡Œé”™è¯¯: {str(e)}\n{traceback.format_exc()}"
    
    return state

def code_optimization_node(state: CodeAssistantState) -> CodeAssistantState:
    """ä»£ç ä¼˜åŒ–å»ºè®®èŠ‚ç‚¹"""
    llm = ChatOpenAI()
    
    code = state["code_context"].get("code", "")
    execution_result = state["execution_output"]
    
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä»£ç ä¼˜åŒ–ä¸“å®¶ã€‚
    åŸå§‹ä»£ç ï¼š
    {code}
    
    æ‰§è¡Œç»“æœï¼š{execution_result}
    
    è¯·æä¾›ä»¥ä¸‹æ–¹é¢çš„ä¼˜åŒ–å»ºè®®ï¼š
    1. æ€§èƒ½ä¼˜åŒ–
    2. ä»£ç å¯è¯»æ€§
    3. é”™è¯¯å¤„ç†
    4. æœ€ä½³å®è·µ
    """
    
    response = llm.invoke([SystemMessage(content=system_prompt)])
    
    return {"messages": [response]}

def should_execute(state: CodeAssistantState) -> str:
    """å†³å®šæ˜¯å¦æ‰§è¡Œä»£ç """
    return "execute" if not state["analysis_result"]["has_syntax_error"] else "skip"

# æ„å»ºä»£ç åŠ©æ‰‹å›¾
builder = StateGraph(CodeAssistantState)
builder.add_node("analysis", code_analysis_node)
builder.add_node("execution", code_execution_node)
builder.add_node("optimization", code_optimization_node)

builder.add_edge(START, "analysis")
builder.add_conditional_edges(
    "analysis",
    should_execute,
    {
        "execute": "execution",
        "skip": "optimization"
    }
)
builder.add_edge("execution", "optimization")
builder.add_edge("optimization", END)

code_assistant_graph = builder.compile()

# æµ‹è¯•ä»£ç åŠ©æ‰‹
test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)

# æµ‹è¯•
print(f"Fibonacci(10) = {fibonacci(10)}")
"""

result = code_assistant_graph.invoke({
    "messages": [HumanMessage(content=f"è¯·åˆ†æä»¥ä¸‹ä»£ç ï¼š\n```python\n{test_code}\n```")],
    "code_context": {},
    "analysis_result": {},
    "execution_output": ""
})

print("ä»£ç åˆ†æç»“æœ:")
print(f"è¯­æ³•æ£€æŸ¥: {result['analysis_result']['error_message']}")
print(f"æ‰§è¡Œç»“æœ: {result['execution_output']}")
print(f"ä¼˜åŒ–å»ºè®®: {result['messages'][-1].content}")
```

### 5.3 ç ”ç©¶åŠ©æ‰‹

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_openai import ChatOpenAI
from langchain_community.tools import TavilySearchResults
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, SystemMessage
import json

class ResearchState(MessagesState):
    research_topic: str
    search_results: list
    research_outline: dict
    draft_sections: dict
    final_report: str

def topic_analysis_node(state: ResearchState) -> ResearchState:
    """ç ”ç©¶ä¸»é¢˜åˆ†æèŠ‚ç‚¹"""
    llm = ChatOpenAI()
    messages = state["messages"]
    last_message = messages[-1]
    
    # æå–ç ”ç©¶ä¸»é¢˜
    if "ç ”ç©¶ä¸»é¢˜:" in last_message.content:
        topic = last_message.content.split("ç ”ç©¶ä¸»é¢˜:")[1].strip()
        state["research_topic"] = topic
        
        # ç”Ÿæˆç ”ç©¶å¤§çº²
        system_prompt = f"""
        è¯·ä¸ºä»¥ä¸‹ç ”ç©¶ä¸»é¢˜ç”Ÿæˆè¯¦ç»†çš„ç ”ç©¶å¤§çº²ï¼š
        {topic}
        
        å¤§çº²åº”åŒ…æ‹¬ï¼š
        1. ç ”ç©¶èƒŒæ™¯
        2. ä¸»è¦é—®é¢˜
        3. ç›¸å…³å­ä¸»é¢˜
        4. é¢„æœŸç»“è®ºæ–¹å‘
        
        ä»¥JSONæ ¼å¼è¿”å›ã€‚
        """
        
        response = llm.invoke([SystemMessage(content=system_prompt)])
        
        try:
            outline = json.loads(response.content)
            state["research_outline"] = outline
        except:
            state["research_outline"] = {"main_topic": topic, "sections": ["èƒŒæ™¯", "æ–¹æ³•", "ç»“æœ", "è®¨è®º"]}
    
    return state

def search_coordination_node(state: ResearchState) -> ResearchState:
    """æœç´¢åè°ƒèŠ‚ç‚¹"""
    # è¿™é‡Œå¯ä»¥é›†æˆå¤šä¸ªæœç´¢å·¥å…·
    search_tool = TavilySearchResults(max_results=3)
    
    topic = state["research_topic"]
    outline = state["research_outline"]
    
    # åŸºäºå¤§çº²ç”Ÿæˆæœç´¢æŸ¥è¯¢
    search_queries = []
    if "sections" in outline:
        search_queries = [f"{topic} {section}" for section in outline["sections"][:3]]
    else:
        search_queries = [topic]
    
    # æ‰§è¡Œæœç´¢
    search_results = []
    for query in search_queries:
        try:
            results = search_tool.invoke(query)
            search_results.extend(results)
        except:
            pass
    
    state["search_results"] = search_results
    return state

def content_generation_node(state: ResearchState) -> ResearchState:
    """å†…å®¹ç”ŸæˆèŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-4")
    
    topic = state["research_topic"]
    outline = state["research_outline"]
    search_results = state["search_results"]
    
    # ç”Ÿæˆç ”ç©¶æŠ¥å‘Š
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆã€‚
    
    ç ”ç©¶ä¸»é¢˜ï¼š{topic}
    ç ”ç©¶å¤§çº²ï¼š{json.dumps(outline, ensure_ascii=False)}
    æœç´¢ç»“æœï¼š{json.dumps(search_results, ensure_ascii=False)}
    
    è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„ç ”ç©¶æŠ¥å‘Šã€‚
    æŠ¥å‘Šåº”åŒ…æ‹¬ï¼š
    1. æ‰§è¡Œæ‘˜è¦
    2. è¯¦ç»†åˆ†æ
    3. å…³é”®å‘ç°
    4. ç»“è®ºå’Œå»ºè®®
    """
    
    response = llm.invoke([SystemMessage(content=system_prompt)])
    state["final_report"] = response.content
    
    return {"messages": [response]}

# æ„å»ºç ”ç©¶åŠ©æ‰‹å›¾
builder = StateGraph(ResearchState)
builder.add_node("topic_analysis", topic_analysis_node)
builder.add_node("search_coordination", search_coordination_node)
builder.add_node("content_generation", content_generation_node)

builder.add_edge(START, "topic_analysis")
builder.add_edge("topic_analysis", "search_coordination")
builder.add_edge("search_coordination", "content_generation")
builder.add_edge("content_generation", END)

research_graph = builder.compile()

# æµ‹è¯•ç ”ç©¶åŠ©æ‰‹
print("ç ”ç©¶åŠ©æ‰‹æµ‹è¯•")
print("=" * 50)

research_request = "ç ”ç©¶ä¸»é¢˜: äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨"

result = research_graph.invoke({
    "messages": [HumanMessage(content=research_request)],
    "research_topic": "",
    "search_results": [],
    "research_outline": {},
    "draft_sections": {},
    "final_report": ""
})

print(f"ç ”ç©¶ä¸»é¢˜: {result['research_topic']}")
print(f"ç ”ç©¶å¤§çº²: {json.dumps(result['research_outline'], ensure_ascii=False, indent=2)}")
print(f"æœç´¢ç»“æœæ•°é‡: {len(result['search_results'])}")
print(f"æŠ¥å‘Šé•¿åº¦: {len(result['final_report'])} å­—ç¬¦")
print(f"æŠ¥å‘Šé¢„è§ˆ: {result['final_report'][:500]}...")
```

---

## 6. æœ€ä½³å®è·µå’Œé«˜çº§ç‰¹æ€§

### 6.1 çŠ¶æ€è®¾è®¡åŸåˆ™

#### 1. çŠ¶æ€ç»“æ„æ¸…æ™°
```python
# å¥½çš„åšæ³• - ç»“æ„æ¸…æ™°
class AppState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    user_info: Dict[str, Any]
    current_task: str
    task_status: str

# é¿å…è¿‡åº¦åµŒå¥—
class ComplexState(TypedDict):
    level1: Dict[str, Dict[str, Any]]  # é¿å…è¿™ç§æ·±å±‚åµŒå¥—
```

#### 2. ä½¿ç”¨åˆé€‚çš„reducer
```python
from typing import Annotated
from langgraph.graph.message import add_messages
from operator import add

class State(TypedDict):
    # æ¶ˆæ¯åˆ—è¡¨ - ä½¿ç”¨add_messagesè‡ªåŠ¨åˆå¹¶
    messages: Annotated[List[BaseMessage], add_messages]
    
    # è®¡æ•°å™¨ - ä½¿ç”¨addè¿›è¡Œç´¯åŠ 
    counter: Annotated[int, add]
    
    # æ™®é€šå­—æ®µ - ç›´æ¥æ›¿æ¢
    status: str
```

#### 3. çŠ¶æ€éªŒè¯
```python
from pydantic import BaseModel, validator

class ValidatedState(BaseModel):
    messages: List[BaseMessage] = []
    user_id: str
    
    @validator('user_id')
    def validate_user_id(cls, v):
        if not v or len(v) < 3:
            raise ValueError('user_id must be at least 3 characters')
        return v
```

### 6.2 é”™è¯¯å¤„ç†ç­–ç•¥

#### 1. èŠ‚ç‚¹çº§é”™è¯¯å¤„ç†
```python
def robust_node(state: State) -> State:
    try:
        # å¯èƒ½å‡ºé”™çš„æ“ä½œ
        result = risky_operation(state["input"])
        return {"result": result, "status": "success"}
    except SpecificError as e:
        # å¤„ç†ç‰¹å®šé”™è¯¯
        return {"error": str(e), "status": "error", "error_type": "specific"}
    except Exception as e:
        # å¤„ç†å…¶ä»–é”™è¯¯
        return {"error": str(e), "status": "error", "error_type": "unknown"}
```

#### 2. æ¡ä»¶è¾¹é”™è¯¯å¤„ç†
```python
def error_handler(state: State) -> str:
    if state.get("status") == "error":
        return "error_node"
    elif state.get("result") is None:
        return "retry_node"
    else:
        return "success_node"

builder.add_conditional_edges(
    "processing_node",
    error_handler,
    {
        "error_node": "error_handler",
        "retry_node": "retry_processor",
        "success_node": "success_handler"
    }
)
```

#### 3. é‡è¯•æœºåˆ¶
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def retryable_node(state: State) -> State:
    # å¸¦é‡è¯•çš„èŠ‚ç‚¹é€»è¾‘
    result = call_external_api(state["request"])
    return {"result": result}
```

### 6.3 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### 1. å¹¶è¡Œæ‰§è¡Œ
```python
from langgraph.graph import StateGraph
import asyncio

async def parallel_node_1(state: State) -> State:
    # å¼‚æ­¥æ“ä½œ
    result = await async_operation_1(state["input1"])
    return {"result1": result}

async def parallel_node_2(state: State) -> State:
    # å¼‚æ­¥æ“ä½œ
    result = await async_operation_2(state["input2"])
    return {"result2": result}

# ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œ
async def run_parallel():
    # å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªèŠ‚ç‚¹
    results = await asyncio.gather(
        parallel_node_1({"input1": "data1"}),
        parallel_node_2({"input2": "data2"})
    )
    return results
```

#### 2. ç¼“å­˜ä¼˜åŒ–
```python
from functools import lru_cache
import hashlib

class CachedState(TypedDict):
    input_hash: str
    cached_result: Any

def create_cache_key(data: Any) -> str:
    """åˆ›å»ºç¼“å­˜é”®"""
    return hashlib.md5(str(data).encode()).hexdigest()

@lru_cache(maxsize=128)
def cached_node_function(input_data: str) -> Any:
    """å¸¦ç¼“å­˜çš„èŠ‚ç‚¹å‡½æ•°"""
    # æ˜‚è´µçš„è®¡ç®—
    return expensive_computation(input_data)

def cached_node(state: State) -> State:
    cache_key = create_cache_key(state["input"])
    
    # æ£€æŸ¥ç¼“å­˜
    if state.get("input_hash") == cache_key:
        return {"result": state["cached_result"], "from_cache": True}
    
    # è®¡ç®—å¹¶ç¼“å­˜
    result = cached_node_function(str(state["input"]))
    return {
        "result": result,
        "input_hash": cache_key,
        "cached_result": result,
        "from_cache": False
    }
```

#### 3. æµå¼å¤„ç†
```python
from langchain_core.callbacks import StreamingStdOutCallbackHandler

def streaming_node(state: State) -> State:
    """æµå¼å¤„ç†èŠ‚ç‚¹"""
    llm = ChatOpenAI(
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    
    # æµå¼ç”Ÿæˆå“åº”
    response = ""
    for chunk in llm.stream(state["input"]):
        response += chunk.content
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ä¸­é—´çŠ¶æ€æ›´æ–°
        
    return {"result": response}
```

### 6.4 è°ƒè¯•å’Œç›‘æ§

#### 1. è¯¦ç»†æ—¥å¿—
```python
import logging
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def logged_node(state: State) -> State:
    """å¸¦è¯¦ç»†æ—¥å¿—çš„èŠ‚ç‚¹"""
    start_time = datetime.now()
    logger.info(f"Node started at {start_time}")
    logger.info(f"Input state: {state}")
    
    try:
        # èŠ‚ç‚¹é€»è¾‘
        result = perform_operation(state)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info(f"Node completed in {duration} seconds")
        logger.info(f"Output: {result}")
        
        return result
    except Exception as e:
        logger.error(f"Node failed: {str(e)}")
        raise
```

#### 2. çŠ¶æ€æ£€æŸ¥ç‚¹
```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph

# åˆ›å»ºå†…å­˜æ£€æŸ¥ç‚¹
memory = MemorySaver()

# æ„å»ºå¸¦æ£€æŸ¥ç‚¹çš„å›¾
builder = StateGraph(State)
# ... æ·»åŠ èŠ‚ç‚¹å’Œè¾¹

graph = builder.compile(checkpointer=memory)

# ä½¿ç”¨æ£€æŸ¥ç‚¹
config = {"configurable": {"thread_id": "user_123"}}

# æ‰§è¡Œå›¾ï¼ŒçŠ¶æ€ä¼šè¢«ä¿å­˜
result = graph.invoke(initial_state, config)

# å¯ä»¥æ¢å¤åˆ°ä¹‹å‰çš„çŠ¶æ€
saved_state = graph.get_state(config)
print(f"Saved state: {saved_state}")
```

#### 3. æ€§èƒ½ç›‘æ§
```python
import time
from dataclasses import dataclass

@dataclass
class PerformanceMetrics:
    node_name: str
    execution_time: float
    memory_usage: int
    state_size: int

def monitored_node(state: State) -> State:
    """å¸¦æ€§èƒ½ç›‘æ§çš„èŠ‚ç‚¹"""
    start_time = time.time()
    start_memory = get_memory_usage()  # è‡ªå®šä¹‰å‡½æ•°
    
    # æ‰§è¡ŒèŠ‚ç‚¹é€»è¾‘
    result = node_logic(state)
    
    # è®°å½•æ€§èƒ½æŒ‡æ ‡
    end_time = time.time()
    end_memory = get_memory_usage()
    
    metrics = PerformanceMetrics(
        node_name="monitored_node",
        execution_time=end_time - start_time,
        memory_usage=end_memory - start_memory,
        state_size=len(str(state))
    )
    
    # å¯ä»¥å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
    send_metrics_to_monitoring_system(metrics)
    
    return result
```

### 6.5 æŒä¹…åŒ–å’Œæ£€æŸ¥ç‚¹

#### 1. å†…å­˜æ£€æŸ¥ç‚¹
```python
from langgraph.checkpoint.memory import MemorySaver

# åˆ›å»ºå†…å­˜æ£€æŸ¥ç‚¹
memory_checkpointer = MemorySaver()

# æ„å»ºå›¾æ—¶æ·»åŠ æ£€æŸ¥ç‚¹
graph = builder.compile(checkpointer=memory_checkpointer)

# ä½¿ç”¨æ£€æŸ¥ç‚¹æ‰§è¡Œ
config = {"configurable": {"thread_id": "conversation_123"}}
result = graph.invoke(state, config)

# è·å–å†å²çŠ¶æ€
history = list(graph.get_state_history(config))
```

#### 2. æ•°æ®åº“å­˜å‚¨æ£€æŸ¥ç‚¹
```python
from langgraph.checkpoint.sqlite import SqliteSaver
import sqlite3

# åˆ›å»ºSQLiteè¿æ¥
conn = sqlite3.connect("checkpoints.db", check_same_thread=False)

# åˆ›å»ºSQLiteæ£€æŸ¥ç‚¹
sqlite_checkpointer = SqliteSaver(conn)

# ä½¿ç”¨SQLiteæ£€æŸ¥ç‚¹æ„å»ºå›¾
graph = builder.compile(checkpointer=sqlite_checkpointer)

# æ‰§è¡ŒåçŠ¶æ€ä¼šè¢«ä¿å­˜åˆ°æ•°æ®åº“
result = graph.invoke(state, config)
```

#### 3. è‡ªå®šä¹‰æ£€æŸ¥ç‚¹
```python
from langgraph.checkpoint.base import BaseCheckpointSaver
from typing import Any, Optional

class CustomCheckpointSaver(BaseCheckpointSaver):
    """è‡ªå®šä¹‰æ£€æŸ¥ç‚¹ä¿å­˜å™¨"""
    
    def __init__(self, storage_backend: Any):
        self.storage = storage_backend
    
    def put(self, config: dict, checkpoint: dict, metadata: dict) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        thread_id = config["configurable"]["thread_id"]
        self.storage.save(thread_id, checkpoint, metadata)
    
    def get(self, config: dict) -> Optional[dict]:
        """è·å–æ£€æŸ¥ç‚¹"""
        thread_id = config["configurable"]["thread_id"]
        return self.storage.load(thread_id)
    
    def list(self, config: dict):
        """åˆ—å‡ºæ£€æŸ¥ç‚¹å†å²"""
        thread_id = config["configurable"]["thread_id"]
        return self.storage.list_checkpoints(thread_id)

# ä½¿ç”¨è‡ªå®šä¹‰æ£€æŸ¥ç‚¹
custom_checkpointer = CustomCheckpointSaver(your_storage_backend)
graph = builder.compile(checkpointer=custom_checkpointer)
```

---

## 7. å­¦ä¹ è·¯å¾„å»ºè®®

### 7.1 åˆå­¦è€…è·¯å¾„ï¼ˆ1-4å‘¨ï¼‰

#### ç¬¬1å‘¨ï¼šåŸºç¡€æ¦‚å¿µ
- **ç›®æ ‡**ï¼šç†è§£LangGraphçš„æ ¸å¿ƒæ¦‚å¿µ
- **å­¦ä¹ å†…å®¹**ï¼š
  - ä»€ä¹ˆæ˜¯å›¾ç»“æ„ã€çŠ¶æ€ã€èŠ‚ç‚¹ã€è¾¹
  - å®‰è£…å’Œç¯å¢ƒé…ç½®
  - Hello Worldç¤ºä¾‹
- **å®è·µé¡¹ç›®**ï¼šåˆ›å»ºä¸€ä¸ªç®€å•çš„é—®å€™æœºå™¨äºº
- **æ¨èèµ„æº**ï¼š
  - LangGraphå®˜æ–¹æ–‡æ¡£
  - æœ¬æŒ‡å—çš„ç¬¬1-3ç« 

#### ç¬¬2å‘¨ï¼šæ ¸å¿ƒç»„ä»¶
- **ç›®æ ‡**ï¼šæŒæ¡Stateã€Nodesã€Edgesçš„ä½¿ç”¨
- **å­¦ä¹ å†…å®¹**ï¼š
  - çŠ¶æ€å®šä¹‰å’Œç®¡ç†
  - åˆ›å»ºä¸åŒç±»å‹çš„èŠ‚ç‚¹
  - è¿æ¥èŠ‚ç‚¹çš„è¾¹
- **å®è·µé¡¹ç›®**ï¼šæ„å»ºä¸€ä¸ªåŸºæœ¬çš„é—®ç­”ç³»ç»Ÿ
- **ç»ƒä¹ **ï¼š
  - å®ç°çŠ¶æ€æ›´æ–°é€»è¾‘
  - åˆ›å»ºæ¡ä»¶åˆ†æ”¯
  - å¤„ç†é”™è¯¯æƒ…å†µ

#### ç¬¬3å‘¨ï¼šå®é™…åº”ç”¨
- **ç›®æ ‡**ï¼šæ„å»ºç¬¬ä¸€ä¸ªå®Œæ•´çš„åº”ç”¨
- **å­¦ä¹ å†…å®¹**ï¼š
  - é›†æˆLLMæœåŠ¡
  - å·¥å…·è°ƒç”¨
  - ç®€å•çš„ç”¨æˆ·äº¤äº’
- **å®è·µé¡¹ç›®**ï¼šæ™ºèƒ½å®¢æœæœºå™¨äºº
- **ç»ƒä¹ **ï¼š
  - å¤„ç†ç”¨æˆ·è¾“å…¥
  - ç”ŸæˆAIå›å¤
  - ç®¡ç†å¯¹è¯çŠ¶æ€

#### ç¬¬4å‘¨ï¼šè¿›é˜¶ç‰¹æ€§
- **ç›®æ ‡**ï¼šæŒæ¡å¾ªç¯å’Œå¤æ‚çŠ¶æ€ç®¡ç†
- **å­¦ä¹ å†…å®¹**ï¼š
  - å¾ªç¯ç»“æ„
  - å¤šè½®å¯¹è¯
  - å¤æ‚çŠ¶æ€è®¾è®¡
- **å®è·µé¡¹ç›®**ï¼šä¸ªäººåŠ©ç†æœºå™¨äºº
- **ç»ƒä¹ **ï¼š
  - å®ç°ä»»åŠ¡ç®¡ç†
  - ç”¨æˆ·åå¥½è®°å¿†
  - ä¸Šä¸‹æ–‡ç†è§£

### 7.2 è¿›é˜¶è·¯å¾„ï¼ˆ1-3ä¸ªæœˆï¼‰

#### ç¬¬1ä¸ªæœˆï¼šé«˜çº§åŠŸèƒ½
- **ç›®æ ‡**ï¼šæŒæ¡LangGraphçš„é«˜çº§ç‰¹æ€§
- **å­¦ä¹ å†…å®¹**ï¼š
  - å­å›¾å’Œæ¨¡å—åŒ–è®¾è®¡
  - æŒä¹…åŒ–å’Œæ£€æŸ¥ç‚¹
  - äººæœºäº¤äº’
  - æµå¼å¤„ç†
- **å®è·µé¡¹ç›®**ï¼šå¤æ‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
- **é¡¹ç›®å»ºè®®**ï¼š
  - ç ”ç©¶å›¢é˜ŸåŠ©æ‰‹
  - ä»£ç å®¡æŸ¥ç³»ç»Ÿ
  - æ•°æ®åˆ†æå·¥ä½œæµ

#### ç¬¬2ä¸ªæœˆï¼šæ€§èƒ½ä¼˜åŒ–
- **ç›®æ ‡**ï¼šä¼˜åŒ–åº”ç”¨æ€§èƒ½å’Œå¯é æ€§
- **å­¦ä¹ å†…å®¹**ï¼š
  - å¹¶è¡Œæ‰§è¡Œ
  - ç¼“å­˜ç­–ç•¥
  - é”™è¯¯å¤„ç†å’Œé‡è¯•
  - ç›‘æ§å’Œè°ƒè¯•
- **å®è·µé¡¹ç›®**ï¼šé«˜æ€§èƒ½ç”Ÿäº§åº”ç”¨
- **ä¼˜åŒ–ç›®æ ‡**ï¼š
  - å“åº”æ—¶é—´ < 2ç§’
  - é”™è¯¯ç‡ < 1%
  - æ”¯æŒå¹¶å‘ç”¨æˆ·

#### ç¬¬3ä¸ªæœˆï¼šç”Ÿäº§éƒ¨ç½²
- **ç›®æ ‡**ï¼šå°†åº”ç”¨éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
- **å­¦ä¹ å†…å®¹**ï¼š
  - éƒ¨ç½²ç­–ç•¥
  - å¯æ‰©å±•æ€§è®¾è®¡
  - å®‰å…¨è€ƒè™‘
  - ç»´æŠ¤å’Œç›‘æ§
- **å®è·µé¡¹ç›®**ï¼šä¼ä¸šçº§åº”ç”¨éƒ¨ç½²
- **éƒ¨ç½²å¹³å°**ï¼š
  - LangGraph Cloud
  - Dockerå®¹å™¨åŒ–
  - äº‘æœåŠ¡é›†æˆ

### 7.3 ä¸“å®¶è·¯å¾„ï¼ˆ3-6ä¸ªæœˆï¼‰

#### æ·±åº¦å®šåˆ¶
- **è‡ªå®šä¹‰èŠ‚ç‚¹ç±»å‹**ï¼šåˆ›å»ºä¸“é—¨çš„èŠ‚ç‚¹ç±»å‹
- **é«˜çº§çŠ¶æ€ç®¡ç†**ï¼šå¤æ‚çš„çŠ¶æ€åŒæ­¥å’Œåˆ†å¸ƒ
- **æ€§èƒ½è°ƒä¼˜**ï¼šæ·±åº¦æ€§èƒ½åˆ†æå’Œä¼˜åŒ–
- **å®‰å…¨åŠ å›º**ï¼šä¼ä¸šçº§å®‰å…¨å®ç°

#### æ¶æ„è®¾è®¡
- **å¾®æœåŠ¡æ¶æ„**ï¼šæ„å»ºå¯æ‰©å±•çš„æ™ºèƒ½ä½“ç³»ç»Ÿ
- **å¤šæ¨¡æ€é›†æˆ**ï¼šç»“åˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘
- **å®æ—¶å¤„ç†**ï¼šæµå¼æ•°æ®å¤„ç†å’Œå“åº”
- **è¾¹ç¼˜è®¡ç®—**ï¼šåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²

#### åˆ›æ–°åº”ç”¨
- **é¢†åŸŸç‰¹åŒ–**ï¼šä¸ºç‰¹å®šè¡Œä¸šå®šåˆ¶è§£å†³æ–¹æ¡ˆ
- **ç ”ç©¶å‰æ²¿**ï¼šæ¢ç´¢æœ€æ–°çš„AIæŠ€æœ¯é›†æˆ
- **å¼€æºè´¡çŒ®**ï¼šå‚ä¸LangGraphç”Ÿæ€å»ºè®¾
- **æŠ€æœ¯åˆ†äº«**ï¼šæ’°å†™æŠ€æœ¯æ–‡ç« å’Œæ¼”è®²

### 7.4 æ¨èèµ„æºå’Œç¤¾åŒº

#### å®˜æ–¹èµ„æº
- **LangGraphæ–‡æ¡£**ï¼šhttps://langchain-ai.github.io/langgraph/
- **GitHubä»“åº“**ï¼šhttps://github.com/langchain-ai/langgraph
- **APIå‚è€ƒ**ï¼šè¯¦ç»†çš„APIæ–‡æ¡£å’Œç¤ºä¾‹

#### å­¦ä¹ å¹³å°
- **LangChain Academy**ï¼šå®˜æ–¹åœ¨çº¿è¯¾ç¨‹
- **YouTubeé¢‘é“**ï¼šLangChainå®˜æ–¹è§†é¢‘æ•™ç¨‹
- **æŠ€æœ¯åšå®¢**ï¼šå®˜æ–¹æŠ€æœ¯åšå®¢å’Œæ›´æ–°

#### ç¤¾åŒºèµ„æº
- **Discordç¤¾åŒº**ï¼šå®æ—¶è®¨è®ºå’Œé—®ç­”
- **GitHubè®¨è®º**ï¼šæŠ€æœ¯è®¨è®ºå’Œé—®é¢˜è§£ç­”
- **Stack Overflow**ï¼šæŠ€æœ¯é—®ç­”å’Œæ”¯æŒ

#### å®è·µé¡¹ç›®
- **å¼€æºé¡¹ç›®**ï¼šå‚ä¸ç›¸å…³çš„å¼€æºé¡¹ç›®
- **æ¯”èµ›å’ŒæŒ‘æˆ˜**ï¼šå‚åŠ AIå’Œç¼–ç¨‹æ¯”èµ›
- **ä¸ªäººé¡¹ç›®**ï¼šæ„å»ºè§£å†³å®é™…é—®é¢˜çš„åº”ç”¨

#### æŒç»­å­¦ä¹ 
- **æŠ€æœ¯ä¼šè®®**ï¼šå‚åŠ AIå’Œè½¯ä»¶å¼€å‘ä¼šè®®
- **åœ¨çº¿è¯¾ç¨‹**ï¼šæŒç»­å­¦ä¹ æ–°çš„æŠ€æœ¯æ ˆ
- **é˜…è¯»è®ºæ–‡**ï¼šå…³æ³¨æœ€æ–°çš„ç ”ç©¶è¿›å±•
- **å®éªŒå’Œæ¢ç´¢**ï¼šä¸æ–­å°è¯•æ–°çš„æƒ³æ³•

---

## ğŸ¯ æ€»ç»“

LangGraphæ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„æ¡†æ¶ï¼Œä¸ºæ„å»ºå¤æ‚çš„AIåº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚é€šè¿‡æœ¬æŒ‡å—çš„å­¦ä¹ ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

1. **ç†è§£æ ¸å¿ƒæ¦‚å¿µ**ï¼šæŒæ¡çŠ¶æ€ã€èŠ‚ç‚¹ã€è¾¹ç­‰åŸºç¡€æ¦‚å¿µ
2. **æ„å»ºå®é™…åº”ç”¨**ï¼šä»ç®€å•çš„èŠå¤©æœºå™¨äººåˆ°å¤æ‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
3. **ä¼˜åŒ–æ€§èƒ½**ï¼šå®æ–½ç¼“å­˜ã€å¹¶è¡Œå¤„ç†ç­‰ä¼˜åŒ–ç­–ç•¥
4. **éƒ¨ç½²ç”Ÿäº§**ï¼šå°†åº”ç”¨éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
5. **æŒç»­æ”¹è¿›**ï¼šé€šè¿‡ç›‘æ§å’Œåé¦ˆä¸æ–­ä¼˜åŒ–

è®°ä½ï¼Œå­¦ä¹ LangGraphæ˜¯ä¸€ä¸ªå¾ªåºæ¸è¿›çš„è¿‡ç¨‹ã€‚ä»ç®€å•çš„ä¾‹å­å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚æ€§ï¼Œæœ€ç»ˆä½ å°†èƒ½å¤Ÿæ„å»ºå‡ºå¼ºå¤§è€Œçµæ´»çš„AIåº”ç”¨ã€‚

### ğŸ”— ç›¸å…³é“¾æ¥

- [LangGraphå®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [LangChainå®˜ç½‘](https://www.langchain.com/)
- [GitHubä»“åº“](https://github.com/langchain-ai/langgraph)
- [ç¤¾åŒºè®ºå›](https://discuss.langchain.dev/)

ç¥ä½ åœ¨LangGraphçš„å­¦ä¹ ä¹‹æ—…ä¸­å–å¾—æˆåŠŸï¼ğŸš€